In this section, we consider some of the works that have already been published regarding packed executables detection. The concern of identifying whether an executable file is packed has gained more interest over the years due to the ability of packers to obfuscate malware detection. Different approaches have been presented in the following articles to solve this issue, let it consists of static or dynamic analysis, being supervised or anomaly-based, or even using data-mining or plot conversion.

Lyda et al. \cite{lyda_using_2007} implemented a tool called Bintropy. Their logic is the following: since the entropy is a measure of the probability to independently predict each number in a series of bytes, a higher entropy score is more likely to imply encrypted or compressed sections and therefore to correspond to a packed executable. Bintropy works by iterating over fixed-length data blocks and sums the frequency of each blockâ€™s byte values to end up with a final score. It proposes two modes of block iterations, the first one computing a score per section and the second scanning the whole file, allowing to check for hidden data. No matter the technique used, only blocks of 256 bytes with more than half of the data not being zero values are considered. Tests have been run over four different datasets (plain text, native, packed and encrypted executable files) in order to come up with an average and highest entropy value for each category. When a new input data comes in, its entropy scores are calculated and compared with the previously generated range of values to fall in the appropriate category. While this implementation is really fast and turns out to be an interesting pre-processing step towards a deeper analysis, it suffers from some limitations. The ratio of false negatives can easily increase when facing big data files with only a few encrypted blocks, significantly decreasing the entropy score. False positives can also happen with valid instruction sequences containing a high degree of variability.

In their paper, Perdisci et al. \cite{perdisci_classification_2008} aimed at implementing an efficient packing detector to only send packed executables to a universal unpacker, and thus save an important amount of processing time. They applied pattern recognition in order to extract 9 features among the PE files. The extracted features are: number of standard and non-standard sections, number of executable sections, number of Readable/Writable/Executable sections, number of entries in the Import Address Table and entropy values of the PE header, code sections, data sections and entire PE file. The experiments were run on a dataset of 5 498 executables : 2 598 packed viruses and 2 900 cleanware among which 669 were packed using 17 different packers. Before testing with machine learning algorithms, they first provided the signature-based tool PEiD \cite{peid} with all packed executables. It had a false negative rate of 30.8\%, which means that more than a thousand packed executables were not detected among which there were 604 packed malware. Therefore, they decided to choose these 1 005 samples to build the test set and used the other 4 493 files for training. Test accuracies were computed using 6 different machine learning classifiers and an entropy threshold. It appeared that all classifiers detected more than 95\% of the packed executables, with the best results obtained when using Neural Networks - 98.91\%. The experiments were run on a 2 GHz Dual Core AMD Opteron and allowed to reach an average time for feature extraction of about 2.82 seconds. Classification time has been considered as negligible since it was in the order of $10^{-3}$ seconds.

Conversion to Byte and Markov plot has been proposed by Kancherla et al. \cite{kancherla_packer_2016}. In addition, they use feature extraction and machine learning algorithms like Support-Vector Machines and Random Forests to determine if and how an executable is packed. The Byte plot conversion is simply achieved by considering each pixel as a byte value, resulting in a gray-scale image where a byte value of 0 is black and 255 is white. Markov plots are generated based on Markov models and use byte conversion in combination with RGB transitions. Three sets of features are extracted from the plots, namely the Intensity, Wavelet-based and Gabor-based sets, each gathering features using specific signal processing heuristics. Tests were performed over a dataset containing 5 000 samples of 9 different packers to which a few unpacked files have been added. The dataset presented 534 features and eventual accuracies varied depending on the plot used and packer predicted. Markov plots performed better when predicting files packed with the packer Themida - 99.05\% - while Byte plots gave better results when predicting files packed with TELock - 97.30\%. Support-Vector Machines have also been used with Markov plots in order to compare performance with the signature-based detection tool PEiD. In general, while it outperformed PEiD by around 7\% for packer detection, performances were the same for packer classification and approached 81\% of precision.

Ugarte-Pedrero et al. \cite{ugarte-pedrero_structural_2011} came up with another approach focused on structural feature-based anomaly detection. The goal is to identify whether a file is packed according to its deviation from normality. They use the term \textit{normality} because the training was performed only on not packed executable files. With the aim to acquire structural information out of the PE files, they selected a combination of 211 features. The features mainly came from the PE header and were also computed using common heuristics, such as the number of specific sections or different entropy values. Then, they applied relevance weights using Information Gain over a dataset made up of 1 000 packed and 1 000 not packed executable files to reduce the features domain to 151 components by removing features having zero value. Each feature thus corresponds to a decimal value - its weight - and is normalised. Anomaly detection can then be applied. When a new executable needs to be classified, the method computes the values of the point in the feature space and then compares it with previously calculated points of the unpacked executables. Comparison is achieved using three distance measures, namely Manhattan Distance, Euclidean Distance and Cosine Similarity. For each method, three combination rules - mean, lowest and highest distance value - are applied in order to end up with a final distance value. The validation was performed over a dataset of 500 not packed and 1 000 packed files using 5-fold cross validation. Therefore, 5 scenarios with a training set of 400 not packed executables and a test set of 100 not packed and 1 000 packed samples were evaluated. After feature extraction, 10 thresholds have been proposed for each \textit{(measure, combination rule)} pair to determine whether a file was packed. Conclusions ended up preferring the Euclidean and Manhattan distances with the mean combination rule which were able to predict more than 99\% of the packed executables while maintaining a false positive ratio lower than 1\%. Distance measures such as Cosine Similarity took too much time to process each executable during analysis.

In contrast, Hubbabilli and Dogra \cite{hubballi_detecting_2016} raised the question of whether supervised or anomaly detection was better for packed file identification. The main issue regarding supervised learning was the dataset. Indeed, while they stated that supervised learning is relatively easy to train, it is difficult to generate good and accurate datasets because of new and custom packers. In answer to their question, they implemented both solutions using the same set of features as Perdisci et al. \cite{perdisci_classification_2008}. The first technique was a semi-supervised model. Using both packed and non-packed samples as input, they generated two models for each category of files by averaging the feature values. Then, when a new input came in, its feature vector was generated and the distance to its nearest model was computed using Euclidean and Mahalanobis distances. Regarding the anomaly detector, it uses only not packed samples for training and creates a cluster as mean of all feature vectors with radius \textit{R}. When a new input needs to be classified, the distance between its feature vector and the cluster center is computed. If it is less than \textit{R}, it is considered as not packed, otherwise it is categorised as packed. Experiments were run over two different datasets, one with both packed and not packed executables, the other with only not packed files where some have been manually packed using 7 different packers. When learning only over packed malware, the anomaly detector outperformed the supervised learning by perfectly predicting packed samples for 6 packer families at its best, against 3 for the supervised learning. When training and testing over the first dataset using the classical 80/20 splitting ratio, the semi-supervised approach achieved 95.32 and 98.97\% detection rates while the anomaly detector offered 96.41 and 97.21\% of precision, using Euclidean and Mahalanobis distance, respectively. 

Finally, Biondi et al. \cite{biondi_effective_2019} decided to face the problem of packing detection and classification using machine learning, feature extraction, time assessment as well as creation of their own ground truths of more than 280 000 labeled samples. Their solution is said to be effective because of its high true positive rate, efficient for its low computational time, and robust as of its effectiveness on freshly new input data. Regarding the set of features used, they first decided to select a rather large amount of 119 features and then apply feature selection to keep only the ones having significant weight in the decision process and low extraction cost. The initial 119 features have been gathered in 6 families, namely Byte Entropy, Entry Bytes, Import Functions, Metadata, Resource and Section. Thanks to Cisco, they were able to generate two different ground truths using three different detection tools : Packerid, Yara rules from VirusTotal and a private tool from Cisco. The first ground truth kept only samples for which all detectors agreed on, while the second restrained to one detector only. They then used three different machine learning algorithms - Naive Bayes, Decision Tree and Random Forests - to assess performance and relevance of their datasets. After parameters tuning, the classifiers have been run over the two datasets with every possible feature combination. They achieved both detection and classification tasks using 5-fold cross-validation. Then, an economical analysis was conducted in order to measure after how much time the algorithms should be retrained to keep an acceptable prediction power. Various conclusions were drawn from these experiments. Training classifiers on large feature sets decreased performance compared to those learning on smaller sets. In contrast, accepting a small decrease in effectiveness of 1-2\% resulted in huge time savings - 17 to 44 times faster. Considering a less complex algorithm like Decision Tree instead of Random Forest was therefore more beneficial, considering time and accuracy, which was proven by economical analysis. In general, simpler algorithms with fewer features offered the best uptime ratios. Experiences also showed the importance and complexity of building a reliable ground truth. It appeared that training on a larger but lower-reliability dataset corresponded to a more realistic packing detection scenario.
