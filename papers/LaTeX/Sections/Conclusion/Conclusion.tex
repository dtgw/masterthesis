The objective of this work was to develop a tool that would be able to detect whether a given malware was packed.
We started by establishing a custom ground truth generator, using an open-source feature extraction tool and various packing detectors in order to build \textit{(features, label)} pairs from samples provided by Cisco. Afterwards, we studied different machine-learning algorithms, fine-tuned and fed them using different combinations of datasets and features in order to improve both test accuracies as well as processing times. While doing this, we kept supplying our database with more and more analysis, these additions being dependent of the frequency at which Cisco sent their malware. When enough samples were collected, we proceeded to economical analysis in order to assess the stability of previously chosen classifiers, namely K-Nearest Neighbors, Logistic Regression, Decision Tree and Random Forest. The deployment of the online tool formalised the completion of the initial objective. Using the Decision Tree classifier, any PE file can be classified with a precision of 99.5\% in less than 50 milliseconds. 

\subsection*{Main findings}
Extensive experiments and testings allowed us to come up with interesting findings. 
We first noticed the importance of having an accurate ground truth. We made the strong hypothesis that the ones we created ourselves delivered the desired results. Therefore, we had to pay particular attention to the different parameters (features used, range of values, threshold for the detector, ...) and find the optimal combination that would offer both the best results while mimicking a real behaviour. Testing of the different scenarios showed that producing an optimal ground truth was indeed a challenge in itself and had a huge impact on further learnings.

In addition to the ground truth generation, we also measured the role of pre-processing. At first, this step was put aside since we thought that keeping the initial ranges of values would allow the different algorithms to capture most of the variances. While this reasoning was applicable to some models like Decision Tree and Ensemble of Decision Trees, we discovered that not applying normalisation or standardisation could however result in hazardous performance, the data being meaningless depending on the model used. Therefore, proceeding to data scaling allowed to improve performance on the test set from 5\% in the poorest case up to 43\% in the best scenario. Moreover, some models like Neural Networks or Linear Models got rid of their convergence issue by simply working with features within the [-1,1] range of values.

Moreover, feature selection processes have also proved to be crucial. When applying feature selection to eligible classifiers, the mean \textit{time-to-accuracy} ratio is about 282.89, which means that intuitively, if we accept a loss of precision of 0.1\%, we can improve the computation time by 28.29\%. Principal Components Analysis also offered some upgrades. As a reminder, we managed to slightly enhance the precision of the KNN classifier by 1.50\% while saving 33\% of computation time. Since we eventually dealt with several tens of thousands of samples, time savings appeared to be crucial.

\subsection*{Limitations \& Further Works}

In this work, we only considered PE file format. A first improvement would be to extend our architecture to accept any kind of executables, not only Windows ones. If we want to stick to our \textit{modus operandi} and keep generating self-made ground truths, then one way to solve this issue would be to find other detectors that handle other kinds of executables.

While detecting the nature of potentially harmful programs is without any doubt a crucial step towards malware identification, being able to identify precisely which packer was used would allow further malware detectors to unpack the compressed file at the start of their analysis. The focus of our work was solely to identify \textit{if} executables were packed and not \textit{how}, but introducing classification could be done without significant changes. In addition, we could identify multi-packed executables. Once the packer of a file is found, we could unpack it accordingly, put in back in the detector and repeat the process iteratively to detect the next layers.

Because of the overhead caused by starting a sandbox, we begun by using static detectors to build our ground truths. By doing so, we managed to keep pace with new incoming samples from Cisco and eventually analysed more than 140 000 executables. Without time and resources restrictions, it seems appropriate to test whether dynamic executions inside a secured environment might lead to more samples labeled as packed and how this would impact further machine learning analysis.

Finally, although an economical analysis was performed over an appropriate period of time, implementing automated re-learning would have been an interesting feature. The idea would be to measure how much time it would take before the average accuracy drops under an acceptable threshold, e.g. 95\%, thereby forcing the classifier to restart its learning phase.

% Finally, we could improve the way we handle errors. When a detector fails to analyse a sample, its associated detection value is set to \textit{error}. When producing our datasets, such results reduce the probability of a file to be packed. Although the generator script allows to consider \textit{errors} as \textit{packed} outputs, this solution isn't realistic. Since at the end such samples represented around 5\% of the total data, using unsupervised machine learning methods like clustering could help to correctly categorise them and enhance the overall precision.