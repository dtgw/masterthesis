{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from utils import PCA_reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance      Training acc    Test acc  Components      Time (s)\n",
      "----------  --------------  ----------  ------------  ----------\n",
      "no pca            0.977393    0.970387  all              57.9509\n",
      "1                 0.998987    0.994389  119              13.5753\n",
      "0.99              0.998909    0.994389  102              10.9482\n",
      "0.95              0.998987    0.994701  82               10.4292\n",
      "0.9               0.998987    0.994077  64               10.0399\n",
      "0.85              0.998597    0.995012  52               10.5986\n"
     ]
    }
   ],
   "source": [
    "PCA_reduction('../../dumps/various_sizes/16K.csv','mlp1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test was conducted with the following caracteristics:\n",
    "- MLPClassifier(solver='adam',hidden_layer_sizes=(100, 100, 100),max_iter=10000)\n",
    "\n",
    "which were obtained by selecting the best results from the previous tests before normalization. As we can see, the time savings are quite different than what we could expect, sometimes it takes more time, sometines it doesn't. Overall performances are quite similar though. Seems therefore honest to combine the features to 52 components in order to save around 25% of time and loose only 0.12% precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance      Training acc    Test acc  Components      Time (s)\n",
      "----------  --------------  ----------  ------------  ----------\n",
      "no pca            0.897334    0.897444  all              71.6606\n",
      "1                 0.997583    0.993766  119              28.9659\n",
      "0.99              0.997505    0.994077  102              27.892\n",
      "0.95              0.997583    0.993454  82               27.6785\n",
      "0.9               0.996804    0.993454  64               26.7483\n",
      "0.85              0.996726    0.993766  52               24.0324\n"
     ]
    }
   ],
   "source": [
    "PCA_reduction('../../dumps/various_sizes/16K.csv','mlp2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters here were chosen after normalization leading to this configuration : \n",
    "- MLPClassifier(solver='sgd',activation='tanh',alpha=0.1,hidden_layer_sizes=(100,50,50),max_iter=1000)\n",
    "\n",
    "which also offer really good performance but as a huge overhead regardint the computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
