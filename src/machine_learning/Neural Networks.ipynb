{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle : In this section we'll focus on one specific type of Deep Learning algorithm, namely multilayer perceptrons. MLPs can be viewed as generalizations of linear models that perform multiple stages of processing to come to a decision.\n",
    "This model has a lot more coefficients (also called weights) to learn: there is one between every input and every hidden unit (which make up the hidden layer), and one between every unit in the hidden layer and the output.\n",
    "After computing a weighted sum for each hidden unit, a nonlinear function is applied to the result, usually the rectifying nonlinearity (also known as rectified linear unit or relu) or the tangens hyperbolicus (tanh). The relu cuts off values below zero, while tanh saturates to –1 for low input values and +1 for high input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.958\n",
      "Accuracy on test set: 0.724\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../dumps/2020.01.13-14.25.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0, max_iter=10000)\n",
    "mlp.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the results, we rather be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solver for weight optimization.\n",
    "- ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n",
    "- ‘sgd’ refers to stochastic gradient descent.\n",
    "- ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n",
    "\n",
    "Note: The default solver ‘adam’ works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, ‘lbfgs’ can converge faster and perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver : lbfgs\n",
      "Accuracy on training set: 0.859\n",
      "Accuracy on test set: 0.843\n",
      "Solver : sgd\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.899\n",
      "Solver : adam\n",
      "Accuracy on training set: 0.972\n",
      "Accuracy on test set: 0.853\n"
     ]
    }
   ],
   "source": [
    "solver = ['lbfgs','sgd', 'adam']\n",
    "for i in solver:\n",
    "    print(\"Solver : %s\" % i)\n",
    "    mlp = MLPClassifier(solver=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could expect, the 'adam' algorithm performs quite well on our dataset. Still, the performance on the test set might be improved by tuning other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function for the hidden layer.\n",
    "- ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
    "- ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "- ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n",
    "- ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function : identity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.877\n",
      "Accuracy on test set: 0.875\n",
      "function : logistic\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : tanh\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : relu\n",
      "Accuracy on training set: 0.972\n",
      "Accuracy on test set: 0.853\n"
     ]
    }
   ],
   "source": [
    "act = ['identity','logistic', 'tanh', 'relu']\n",
    "for i in act:\n",
    "    print(\"function : %s\" % i)\n",
    "    mlp = MLPClassifier(activation=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the 'identity' activation didn't manage to make the algorithm converge. For the others, both 'logistic' and 'tanh' provided sam results and performed better on the test set than the training set, which is the opposite for the 'relu' activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate schedule for weight updates (only for 'sgd' solver).\n",
    "- ‘constant’ is a constant learning rate given by ‘learning_rate_init’.\n",
    "- ‘invscaling’ gradually decreases the learning rate at each time step ‘t’ using an inverse scaling exponent of ‘power_t’. effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
    "- ‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is on, the current learning rate is divided by 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function : constant\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.899\n",
      "function : invscaling\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.899\n",
      "function : adaptive\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.899\n"
     ]
    }
   ],
   "source": [
    "learning_rate = ['constant','invscaling', 'adaptive']\n",
    "for i in learning_rate:\n",
    "    print(\"function : %s\" % i)\n",
    "    mlp = MLPClassifier(solver='sgd', learning_rate=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 'sgd' solver, the learning rate doesn't really matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 penalty (regularization term) parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : 0.0001\n",
      "Accuracy on training set: 0.972\n",
      "Accuracy on test set: 0.853\n",
      "alpha : 0.001\n",
      "Accuracy on training set: 0.972\n",
      "Accuracy on test set: 0.849\n",
      "alpha : 0.1\n",
      "Accuracy on training set: 0.967\n",
      "Accuracy on test set: 0.840\n",
      "alpha : 1\n",
      "Accuracy on training set: 0.971\n",
      "Accuracy on test set: 0.853\n",
      "alpha : 10\n",
      "Accuracy on training set: 0.973\n",
      "Accuracy on test set: 0.848\n",
      "alpha : 100\n",
      "Accuracy on training set: 0.932\n",
      "Accuracy on test set: 0.868\n",
      "alpha : 1000\n",
      "Accuracy on training set: 0.906\n",
      "Accuracy on test set: 0.892\n"
     ]
    }
   ],
   "source": [
    "alpha = [0.0001,0.001,0.1,1,10,100,1000]\n",
    "for i in alpha:\n",
    "    print(\"alpha : %s\" % i)\n",
    "    mlp = MLPClassifier(alpha=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more we increase the *alpha* value, the more we reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameter allows us to set the number of layers and the number of nodes we wish to have in the Neural Network Classifier. Each element in the tuple represents the number of nodes at the ith position where i is the index of the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer size : 50 50 50\n",
      "Accuracy on training set: 0.939\n",
      "Accuracy on test set: 0.873\n",
      "layer size : 100 100 100\n",
      "Accuracy on training set: 0.950\n",
      "Accuracy on test set: 0.867\n",
      "layer size : 50 100 50\n",
      "Accuracy on training set: 0.911\n",
      "Accuracy on test set: 0.889\n",
      "layer size : 100 50 50\n",
      "Accuracy on training set: 0.931\n",
      "Accuracy on test set: 0.878\n",
      "layer size : 50 50 100\n",
      "Accuracy on training set: 0.921\n",
      "Accuracy on test set: 0.885\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0265a44cc8e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhidden_layers_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden_layers_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"layer size : %s %s %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "hidden_layers_size = [(50,50,50), (100,100,100), (50,100,50), (100,50,50), (50,50,100), (100,)]\n",
    "for i in hidden_layers_size:\n",
    "    print(\"layer size : %s %s %s\" % (i[0], i[1], i[2]))\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['lbfgs','sgd','adam'], 'max_iter': [1000,10000], 'alpha': [0.0001,0.001,0.1,1,10,100], 'hidden_layer_sizes':[(50,50,50), (100,100,100), (50,100,50), (100,50,50), (50,50,100), (100,)], 'activation':['identity','logistic', 'tanh', 'relu']}\n",
    "clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "clf.fit(data_train, target_train)\n",
    "print(clf.score(data_train, target_train))\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['lbfgs','sgd','adam'], 'max_iter': [1000,10000], 'alpha': [0.0001,0.001,0.1,1,10,100], 'hidden_layer_sizes':[(50,50,50), (100,100,100), (50,100,50), (100,50,50), (50,50,100), (100,)], 'activation':['identity','logistic', 'tanh', 'relu']}\n",
    "clf = RandomizedSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "clf.fit(data_train, target_train)\n",
    "print(clf.score(data_train, target_train))\n",
    "print(clf.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
