{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9a607ad1509a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthomas_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCA_reduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtoBoolean\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tabulate import tabulate\n",
    "from joblib import dump, load\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "sys.path.append('../')\n",
    "from utils import feature_selection, thomas_parser, PCA_reduction, perf, time_comparison\n",
    "from toBoolean import convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('../../../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=4,min_samples_split=0.1,min_samples_leaf=10,random_state=0)\n",
    "tree.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands allow us to see how important each feature is in the decision process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(tree.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = data.shape[1]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "plt.barh(range(n_features), tree.feature_importances_, align='center') \n",
    "plt.yticks(np.arange(n_features), np.arange(1,120)) \n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, out of 119 features only a very few are actually used by the learning algorithm. Let's apply some pruning on the features to see if it improves the accuracies (or even the speed of the process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_selection('../../../dumps/2020.02.10-12.14.csv',0.15,\"tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite intuitive to see that there's no difference between the two first situations. Indeed, since not a lot of features are used anyway, doing a feature selection doesn't improve the accuracies. What is interesting to see is that on the last graph, the number of features used is even lower (between 2 and 3) but the results are nearly the same, which means that just a few features are doing most of the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the performance can be improved when using a larger dataset which is nearly twice the size of the previous one (14703 entries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection('../../../dumps/2020.03.11-17.39.csv',0.15,\"tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suffer the issue of not having enough features between the iterations to create an intersect with more than one value in common. Let's just see how it goes without the intersect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection('../../../dumps/2020.03.11-17.39.csv',0.15,\"tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What shows up straight away is that by simply doubling the size of the dataset we went from a pure test accuracy of 0.92 up to 0.979 (with 90% training set), this is quite impressive ! We can also notice that there's no difference between the results we get with and without the feature extraction, which mean that the model could definittely be simplified to a few features. One last thing we can also observe is that we suffer from a huge drop when setting a value of 0.6 for the training set, which I cannot explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
