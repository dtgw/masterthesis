{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from utils import PCA_reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the data for PCA. PCA is effected by scale so we need to scale the features in the data before applying PCA. We can transform the data onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. StandardScaler helps standardize the datasetâ€™s features.\n",
    "Notice the code below has .95 for the number of components parameter. It means that scikit-learn choose the minimum number of principal components such that 95% of the variance is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Variance    Training acc    Test acc    Components    Time (s)\n",
      "----------  --------------  ----------  ------------  ----------\n",
      "      1           0.984867    0.979939           119    0.175548\n",
      "      0.99        0.984867    0.977899            98    0.171339\n",
      "      0.95        0.984186    0.977219            77    0.170872\n",
      "      0.9         0.984526    0.978579            60    0.137358\n",
      "      0.85        0.984696    0.978239            47    0.128116\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../../dumps/2020.03.11-17.39.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_train)\n",
    "data_train_raw = scaler.transform(data_train)\n",
    "data_test_raw = scaler.transform(data_test)\n",
    "cell_text = []\n",
    "for i in [1,0.99,0.95,0.90,0.85]:\n",
    "    row = []\n",
    "    row.append(i)\n",
    "    start = time.time()\n",
    "    pca = PCA(i) if i != 1 else PCA()\n",
    "    pca.fit(data_train_raw)\n",
    "    data_train = pca.transform(data_train_raw)\n",
    "    data_test = pca.transform(data_test_raw)\n",
    "    clf = KNeighborsClassifier(n_neighbors=6,p=2)\n",
    "    clf.fit(data_train, target_train)\n",
    "    end = time.time()\n",
    "    row.append(clf.score(data_train, target_train))\n",
    "    row.append(clf.score(data_test, target_test))\n",
    "    row.append(pca.n_components_)\n",
    "    row.append(end-start)\n",
    "    cell_text.append(row)\n",
    "print(tabulate(cell_text, headers = ['Variance','Training acc','Test acc','Components','Time (s)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to notice that we can nearly keep the same accuracies while gathering our features from a set of 119 to 47 and clearly improves the timing. Here we're using quite a small dataset (14k), but it makes no doubt that saved time is significant with even more samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just test without standardization and with normalization in order to see how performances are impacted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.96\n",
      "Test set accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../../dumps/2020.03.11-17.39.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "pca = PCA(0.95)\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "\n",
    "pca.fit(data_train)\n",
    "data_train = pca.transform(data_train)\n",
    "data_test = pca.transform(data_test)\n",
    "clf = KNeighborsClassifier(n_neighbors=6,p=2)\n",
    "clf.fit(data_train, target_train)\n",
    "print(\"Training set accuracy: {:.2f}\".format(clf.score(data_train, target_train)))\n",
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.97\n",
      "Test set accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../../dumps/2020.03.11-17.39.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "pca = PCA(0.95)\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = Normalizer()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "pca.fit(data_train)\n",
    "data_train = pca.transform(data_train)\n",
    "data_test = pca.transform(data_test)\n",
    "clf = KNeighborsClassifier(n_neighbors=6,p=2)\n",
    "clf.fit(data_train, target_train)\n",
    "print(\"Training set accuracy: {:.2f}\".format(clf.score(data_train, target_train)))\n",
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performances are better when we apply Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
