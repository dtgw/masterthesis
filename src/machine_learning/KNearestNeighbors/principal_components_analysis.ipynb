{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from utils import PCA_reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the data for PCA. PCA is effected by scale so we need to scale the features in the data before applying PCA. We can transform the data onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. StandardScaler helps standardize the datasetâ€™s features.\n",
    "Notice the code below has .95 for the number of components parameter. It means that scikit-learn choose the minimum number of principal components such that 95% of the variance is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Variance    Training acc    Test acc    Components    Time (s)\n",
      "----------  --------------  ----------  ------------  ----------\n",
      "      1           0.99314     0.99283            119    0.200065\n",
      "      0.99        0.99314     0.993454           102    0.187137\n",
      "      0.95        0.993296    0.993454            82    0.19538\n",
      "      0.9         0.99314     0.993454            64    0.165704\n",
      "      0.85        0.99314     0.993454            52    0.143395\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../../dumps/various_sizes/16K.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_train)\n",
    "data_train_raw = scaler.transform(data_train)\n",
    "data_test_raw = scaler.transform(data_test)\n",
    "cell_text = []\n",
    "for i in [1,0.99,0.95,0.90,0.85]:\n",
    "    row = []\n",
    "    row.append(i)\n",
    "    start = time.time()\n",
    "    pca = PCA(i) if i != 1 else PCA()\n",
    "    pca.fit(data_train_raw)\n",
    "    data_train = pca.transform(data_train_raw)\n",
    "    data_test = pca.transform(data_test_raw)\n",
    "    clf = KNeighborsClassifier(n_neighbors=7,p=1)\n",
    "    clf.fit(data_train, target_train)\n",
    "    end = time.time()\n",
    "    row.append(clf.score(data_train, target_train))\n",
    "    row.append(clf.score(data_test, target_test))\n",
    "    row.append(pca.n_components_)\n",
    "    row.append(end-start)\n",
    "    cell_text.append(row)\n",
    "print(tabulate(cell_text, headers = ['Variance','Training acc','Test acc','Components','Time (s)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to notice that we can keep (even slightly improve) the accuracies while gathering our features from a set of 119 to 52 and clearly improve the timing. Here we're using quite a small dataset (16k), but it makes no doubt that the saved time is significant with even more samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just test without standardization and with normalization in order to see how performances are impacted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.97\n",
      "Test set accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../../dumps/various_sizes/16K.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "pca = PCA(0.95)\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "\n",
    "pca.fit(data_train)\n",
    "data_train = pca.transform(data_train)\n",
    "data_test = pca.transform(data_test)\n",
    "clf = KNeighborsClassifier(n_neighbors=7,p=1)\n",
    "clf.fit(data_train, target_train)\n",
    "print(\"Training set accuracy: {:.2f}\".format(clf.score(data_train, target_train)))\n",
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.98\n",
      "Test set accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../../dumps/various_sizes/16K.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "pca = PCA(0.95)\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = Normalizer()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "pca.fit(data_train)\n",
    "data_train = pca.transform(data_train)\n",
    "data_test = pca.transform(data_test)\n",
    "clf = KNeighborsClassifier(n_neighbors=7,p=1)\n",
    "clf.fit(data_train, target_train)\n",
    "print(\"Training set accuracy: {:.2f}\".format(clf.score(data_train, target_train)))\n",
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performances are better when we apply Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
